<HTML>
  <HEAD>
	<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    	<TITLE>Vilas Kumar Chitrakaran - CPSC 805: Advanced Computer Graphics</TITLE>
  </HEAD>


<body bgcolor="#3b383d" text="#dae7ef" link="#90b2cb" vlink="#90b2cb">

<hr align="left" width="640" color="#8687bf">
<table width="640" bgcolor="#8687bf">
<tr><td>
<CENTER>
<font color="white"><b>CPSC 805 - Advanced Computer Graphics</b></font>
</CENTER>
</td></tr>
</table>
<hr align="left" width="640" color="#8687bf">

<br><br>

<table width="640">
<tr>
<td width="640">

<div style="text-align:justify;">
<FONT COLOR="dae7ef" FACE="Arial,Helvetica" SIZE="-1">

<table width="640" bgcolor="#8687bf">
<tr><td>
<font color="white" face="times"><b><a name="terraview">Terraview: A mountain terrain generator</a></b></font>
</td></tr>
</table>

<br><br>
<p>
This is a mountain terrain generator based on the Diamond-Square algorithm for recursive mesh subdivision. 
The following are some images generated by my implementation of the algorithm using OpenGL for rendering. 
Each of these images were generated from a mesh containing about 66049 vertices (subdivision depth of 8 = 
[2<sup>8</sup>+1] * [2<sup>8</sup>+1] vertices). The water and sky textures were generated in 
<a href="http://www.planetside.co.uk/terragen/">Terragen</a>.
</p> 

<br><br>
<center>
<img src="images/terraview/terra2.jpg" width="640" height="480" border="1" alt="image1"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image t.1: A rocky shoreline</b></font></font>
</center>
<br><br>

<center>
<img src="images/terraview/terra_snow1.jpg" width="640" height="455" border="1" alt = "Bay area">
<font face="Arial,Helvetica"><font size=-1><b>Image t.2: The Bay. An image produced by a different set of parameters</b></font></font>
</center>
<br><br>

<center>
<img src="images/terraview/terra_snow2.jpg" width="639" height="456" border="1" alt="snow">
<font face="Arial,Helvetica"><font size=-1><b>Image t.3: A closer look at snow around the peaks.</b></font></font>
</center>
<br><br>

<center>
<img src="images/terraview/terra1.jpg" width="640" height="480" border="1" alt="another view">
<font face="Arial,Helvetica"><font size=-1><b>Image t.4: Yet another view.</b></font></font>
</center>
<br><br>

<center>
<img src="images/terraview/terra3.jpg" width="640" height="480" border="1">
<font face="Arial,Helvetica"><font size=-1><b>Image t.5: Top view.</b></font></font>
</center>
<br><br>

<center>
<img src="images/terraview/terra_mesh.jpg" width="640" height="480" border="0">
<font face="Arial,Helvetica"><font size=-1><b>Image t.6: The underlying mesh.</b></font></font>
</center>
<br><br>

<table width="640" bgcolor="#8687bf">
<tr><td>
<font color="white" face="times"><b><a name="finalKiran">Kiran: A Ray-Tracer</a></b></font>
</td></tr>
</table>

<br><br>
<h5>Introduction:</h5>
<p>
This is the first version of a basic ray-tracer <b>Kiran</b> (Sanskrit for a 'ray of light'). The program demonstrates the following capabilities:

 <ul>
 <li> Ambient, specular and diffuse lighting effects with multiple light sources.
 <li> Soft shadows.
 <li> Reflections and Refractions.
 <li> Bump mapping.
 <li> Texture mapping.
 <li> PPM image handling routines.
 <li> A scene specification file format.
 <li> Quadrics.
 <li> Depth of field
 <li> Arbitrary camera placement (position and orientation).
 </ul>
</p>

<br>
<h5>Program Compilation:</h5>
<p>
Type <font size="+1"><code>make</code></font> to compile and create the executable <font size="+1"><code>kiran</code></font>. 
Type <font size="+1"><code>./kiran -i <i>scenefile.env</i></code></font> to run the program and create the rendered scene as <i>output.ppm</i>. 
The following command line switches can also be used:
 <ul>
 <li> -o: Specify a name for the output PPM file (instead of the default <i>output.ppm</i>).
 <li> -i: Specify input scene(.env) file
 </ul>
</p>

<br>
<h5>Program Design Overview:</h5>
<p>
<b>Kiran</b> is modular in design. The ray-tracer has individual components such as lights, 
camera, and 3D objects. Each component is implemented as a C++ class with modifiable attributes. 
There are also data structures and functions to work with 3D vectors and light rays. 
The following are the important data structures present in the current implementation:
 <ul>
 <li><font size="+1"><code>class Pixmap</code></font>: <i>PPM image handling routines.</i>
 <li><font size="+1"><code>class Camera</code></font>: <i>A camera based on the pin hole camera model.</i>
 <li><font size="+1"><code>class Object</code></font>: <i>A pure virtual base class for 3D objects.</i>
  <ul>
  <li><i>Derived classes for specific objects such as spheres, cones, planes.</i>
  </ul>
 <li><font size="+1"><code>class Light</code></font>: <i>A pure virtual base class for all kinds of lights.</i>
  <ul>
  <li><font size="+1"><code>class PointLight</code></font>: <i>Derived class for omni-directional light.</i>
  <li><font size="+1"><code>class AmbientLight</code></font>: <i>Derived class for ambient lights.</i>
  </ul>
 </ul>
</p>

<p>
The description of a 3D scene is internally created as a array of pointers of type <font size="+1"><code>class Object</code></font> 
and an array of pointers of type <font size="+1"><code>class Light</code></font> for all 3D objects and light sources in the 
scene, respectively.
</p>

<p>
<i>Scene Description File format:</i> Input to the ray-tracer is a scene description file 
containing multiple sections that describe the objects in the scene, and their properties. An example is shown below for 
a <font size="+1"><code>Sphere</code></font> object.<br><br>
<font size="+1"><code>
< Sphere ><br>
name blob<br>
position -0.01 -0.01 1<br>
radius 0.03<br>
color 220 0 0<br>
ambient 0.1<br>
diffuse 1<br>
phong 2<br>
phong_size 40<br>
transmittivity 1<br>
refractive_index 1.1<br>
< /Sphere ><br>
</code></font> 
</p>

<p>
Global settings such as image size, enabling/disabling antialiasing can also be specified through the scene file. 
</p>

<br><br>
<img src="images/kiran/twoLights.jpg" width="640" height="480" border="1" alt="the basic ray tracer output with two lights"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.1: The first image from my ray-tracer. Includes shadows, ambient, specular and diffuse lighting.</b></font></font>
<br><br>


<p>
<i>Refractions:</i> Implementing refraction of light through objects took quite an effort. 
Firstly, computation of the direction of refracted ray through the object requires the refractive indices of media on 
both sides of the surface to be passed to the ray-trace routine. I could not initially think of a neat method to pass this value around. (I could have passed
this value along with the incident ray, but the refractive index is the property of an object, not a ray).
Secondly, the direction of surface normals needed to be inverted depending upon whether the ray was entering the refracting object, or leaving it.   
</p>
<p>
The first problem was solved by assuming that if the incident ray is not coming from another object, then it is coming 
through air (with a refractive index of 1). A static variable was used to keep track of the changing refractive 
indices through subsequent refractions. (The refractive index is a property of the object and is read by the 
trace function when it detects an intersection of a ray with an object). Also, it was assumed that if two subsequent 
intersections with the same object by a ray are detected, the ray is leaving the object on the second instance of the intersection, and
the refractive index of the incident ray was reset to 1. At the second intersection, the direction of the surface normal was inverted 
in order to compute the surface normal, because of the way the equation for refracted ray was derived. 
</p>

<br><br>
<center>
<img src="images/kiran/fisrt.jpg" width="640" height="480" border="1" alt="first image from intermediate ray tracer"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.2: Image displaying reflections, refractions, texture mapping and bump mapping.</b></font></font>
</center>
<br><br>

<br><br>
<center>
<img src="images/kiran/diffRefr.jpg" width="348" height="194" border="1" alt="Different refractive indices"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.3: The translucent sphere with different refractive indices (a) air = 1, and (b) water = 1.33</b></font></font>
</center>
<br><br>

<p>
<i>Bump Mapping:</i> To simulate roughness in the surface of objects, I used a ppm image file as a bump-map. The 
'bumpiness' is encoded in the image as pixel intensities. instead of texture mapping the intensities on the surface of the 
object, the intensity values in the bump-map was used to change the 'direction' of the surface normals (as opposed to 
perturbing the coordinates of ray intersection point on the object or the coordinates of the surface normals). In brief, 
the idea is as follows. When an intersection of a ray of light with an object is detected, the 3D point of intersection on the
object is inverse mapped to a 2D coordinate (u,v) in the bump map file. The intensities of pixels around this image (bump map) coordinate
(u,v) are used to scale the magnitude of surface normals around the original 3D intersection point on the object. Adding up all the 
scaled normals around the intersection point along with the normal at the intersection gives a new normal vector that is 
slanted in the direction of maximum change of intensity around (u,v) in the bump map file. This new normal vector is now used for 
lighting calculations. The result is shown in the rendering of the object with the texture of a golf ball in the first image in this section. A portion of
the bump map file that was used is shown below. 
</p>
<br><br>
<center>
<img src="images/kiran/noise.jpg" width="200" height="199" border="0" alt="bump map"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.4: The bump map for the golf ball in the first image.</b></font></font>
</center>
<br><br>

<center>
<img src="images/kiran/sky_a.jpg" width="640" height="480" border="0" alt="un-antialised"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.5: A texture mapped quad, skybox, glossy reflective sphere...</b></font></font>
</center>
<br><br>

<center>
<img src="images/kiran/sky_aa.jpg" width="640" height="480" border="0" alt="antialised"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.6: The same image as above, anti-aliased.</b></font></font>
</center>
<br><br>

<p>
<i>Anti-aliasing:</i> This version of the ray-tracer implements a simple anti-aliasing technique. Each pixel is sampled 5 times -  
one at each of the corners and at the center. The final intensity of each pixel is a weighted sum of all five samples, with a weight 
of 0.5 assigned to the sample at the center of the pixel and a weight of 0.125 for each of the four samples from the corners.  
</p>

<p>
<i>Depth of field:</i> The depth-of-field effect was achieved by simulating a lens surface in front of the image plane. Rays from many 
points along the lens surface within the aperture area are shot into the same focal distance in the object space, and the color returned
by all rays are averaged out. Hence any object at the focal distance appears sharp in the rendered image, while objects nearer and
farther away appear blurred. The depth of field is controlled by aperture and focal distance settings just as in a real camera. 
Larger f-stops (f/2.4, f/5.6...) mean smaller depth of field (only objects at the focal distance appear sharp) while 
smaller f-stops (i.e. f/16, f/32..) mean larger depth of field and many more objects appear to be in focus.  
</p>

<br><br>
<center>
<img src="images/kiran/dof.jpg" width="640" height="247" border="1" alt="depth of field"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.7: Demonstration of Depth of Field. 
<br>Camera focal length </b><i>f</i> = 50 mm<b> and aperture of </b>f/5.6<b>.
<br>Focal distance: Left = 0.5 m, center = 1.5 m and right = 2.5 m.</b></font></font>
</center>
<br><br>

<p>
<i>Soft shadows:</i> Random perturbation of light sources and multiple shadow rays were used in order to simulate soft shadows. 
</p>

<br><br>
<center>
<img src="images/kiran/softs.jpg" width="522" height="300" border="1" alt="soft shadows"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.8: Demonstration of soft shadows. The image on the left 
was created with soft shadows disabled. The image on the right was created with 100 shadow rays per intersection. (no anti-aliasing in either images).</b></font></font>
</center>
<br><br>

<p>
<i>Arbitrary camera placement:</i> This version of the ray-tracer includes an advanced camera model that is closer in behavior to 
real 35mm SLR cameras compared to the pin-hole model used in previous versions. The camera model takes into account effects of   
focal length and lens aperture, both of which are user adjustable parameters.
The camera can be arbitrarily placed anywhere in the global coordinate system through specification of a position vector, look-at vector 
and an up-vector, just as in OpenGL.
</p>

<br><br><br>
<center>
<img src="images/kiran/cam_angle.jpg" width="640" height="325" border="1" alt="Camera positioning"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.9: Demonstration of arbitrary camera movements.</b></font></font>
</center>
<br><br>

<p>
<i>Photon mapping</i>: Photon mapping is a global illumination technique that can realistically simulate effects 
such as caustics (focussed light reflections from a swimming pool or around refractive objects) 
and diffuse inter-reflections (color bleeding between close surfaces). I enhanced the 
ray tracer with photon mapping to add indirect illumination and caustics. 
Here is an image of the mandatory Cornell box produced by my enhanced ray-tracer. 
</p>

<br><br>
<center>
<img src="images/kiran/fullIllum.jpg" width="640" height="480" border="1" alt="photon mapped final"><br>
<font face="Arial,Helvetica"><font size=-1><b>Image k.10: A rendering of the Cornell box with my enhanced ray-tracer. 
Notice the caustic generated by photon mapping under the sphere on the right.</b></font></font>
</center>
<br><br>

<p>
&nbsp;
</p>

</div>
<hr align="left" width="640" color=#8687bf>
<a href="http://www.ces.clemson.edu/~cvilas/index.htm" style="text-decoration:none">Vilas kumar Chitrakaran</a>, Feb. 10, 2004.
</FONT>
<br><br>
</td>
</tr>
</table>
</BODY>
</HTML>
